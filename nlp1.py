'''
*tokenizaation=>
divide given text by words or sentce
*stop word
===a,an,the etc
stopwords.words('engglish')==stopword list

*stemming==
remove stemmers by removing suffix and prefix like ing er s etc
eg-helps to help,imporatnce to important

*lemmitization-
get root word using POS..like saw to see or identityfying it verb or noun
lemmatizer.lemmatize("loving")#loving as noun
lemmatizer.lemmatize("loving","v")#love as verb

*POS Tagging:
gives pos tags for each words in sentence

*NER:named entity recognization
name of place,person organtns etc

'''
